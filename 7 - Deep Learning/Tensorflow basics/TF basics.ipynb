{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Tensorflow basics</div>\n",
    "\n",
    "This notebook contains a gentle introduction to the [low-level API of Tensorflow](https://www.tensorflow.org/programmers_guide/low_level_intro) (built with Tensorflow 1.8).\n",
    "\n",
    "Nice list of extra resources on [Aymeric Damien's github](https://github.com/aymericdamien/TensorFlow-Examples).\n",
    "\n",
    "1. [A plain regression problem](#sec1)\n",
    "2. [Introducing minibatches](#sec2)\n",
    "3. [What if I want to define the gradients myself?](#sec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec1\"></a>1. A plain regression problem\n",
    "\n",
    "We want to train a neural network to learn the square root function over the $[0,10]$ interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.arange(0,10,0.1).reshape((100,1))\n",
    "y_data = np.sqrt(x_data)\n",
    "plt.plot(x_data,y_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a TF session (we could actually do that after defining the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network. 2 hidden layers with 20 units each, ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "hidden_layer1 = tf.layers.dense(x, units=20, activation=tf.nn.relu)\n",
    "hidden_layer2 = tf.layers.dense(hidden_layer1, units=20, activation=tf.nn.relu)\n",
    "y_pred = tf.layers.dense(hidden_layer2, units=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize layers' weights (warning, only initializes variables that exist when the global_variables_initializer() function is called)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a first prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(y_pred, feed_dict={x: x_data[0:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred) # Fill the options\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train network for 10000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_epochs = 10000\n",
    "losses = np.zeros((nb_epochs))\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    _, loss_value = sess.run((train, loss), feed_dict={x: x_data, y_true: y_data})\n",
    "    losses[i] = loss_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss evolution and final learned function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(losses)\n",
    "\n",
    "new_pred = sess.run(y_pred, feed_dict={x: x_data})\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x_data, y_data)\n",
    "plt.plot(x_data, new_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. Introducing minibatches\n",
    "\n",
    "Let's train a softmax linear regression on the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize cross-entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses = np.zeros(training_epochs)\n",
    "accuracies = np.zeros(training_epochs)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "        # Compute average loss\n",
    "        losses[epoch] += l / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(losses[epoch]))\n",
    "    \n",
    "    # Test accuracy on the first 3000 test samples\n",
    "    accuracies[epoch] = accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}, session=sess)\n",
    "\n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(training_epochs),losses)\n",
    "plt.figure()\n",
    "plt.plot(range(training_epochs),accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a> 3. What if I want to define the gradients myself?\n",
    "\n",
    "In some cases, you'll want to define the weight updates (gradients) yourself, rather than relying on the automated differenciation of a loss function.\n",
    "\n",
    "Why? Because sometimes the loss function is not easily defined by you still have a parameter update rule.\n",
    "\n",
    "As a toy example, let's take the previous example and define the gradient ourselves. But since we're lazy and this is just a toy example, we'll use the `tf.gradients` function to define these gradients.\n",
    "\n",
    "It all begins exactly the same way as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the part where we don't want to define a loss function but rather provide gradients. Here we cheat by still defining `loss` but we will just use it as an intermediate variable to call `tf.gradients`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "grad_W, grad_b = tf.gradients(xs=[W, b], ys=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have avoided cheating by writing the derivation of `loss` with respect to all elements in `W` and `b` by hand and obtaining:\n",
    "\n",
    "`grad_W =  - tf.matmul ( tf.transpose(x) , y - pred) \n",
    "grad_b = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)`\n",
    "\n",
    "But we're lazy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still want to use Tensorflow's `GradientDescentOptimizer` to perform the update operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "update_weights = optimizer.apply_gradients(zip((grad_W, grad_b),(W,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses = np.zeros(training_epochs)\n",
    "accuracies = np.zeros(training_epochs)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        _, l = sess.run([update_weights, loss], feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "        # Compute average loss\n",
    "        losses[epoch] += l / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(losses[epoch]))\n",
    "    \n",
    "    # Test accuracy on the first 3000 test samples\n",
    "    accuracies[epoch] = accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}, session=sess)\n",
    "\n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(training_epochs),losses)\n",
    "plt.figure()\n",
    "plt.plot(range(training_epochs),accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's also possible to totally get rid of the `GradientDescentOptimizer` by defining the operations ourself. We can define the two following tensors:\n",
    "\n",
    "`update_W = W.assign(W - learning_rate * grad_W)\n",
    "update_b = b.assign(b - learning_rate * grad_b)`\n",
    "\n",
    "And replace the `_, l = sess.run([update_weights, loss], feed_dict={x: batch_xs, y: batch_ys})` line above by:\n",
    "\n",
    "`_, _, l = sess.run([update_W, update_b, loss], feed_dict={x: batch_xs, y: batch_ys})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
